{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zajęcia 1 (część 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytanie danych\n",
    "\n",
    "Bedziemy wykorzystywać danye z ankiety StackOverflow z 2020.\n",
    "\n",
    "https://insights.stackoverflow.com/survey/\n",
    "\n",
    "Dane sa dostepne na google drive. Skorzystamy z modułu GoogleDriveDownloader, ktory pozwala pobrac dokument o podanym id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googledrivedownloader in /opt/conda/miniconda3/lib/python3.11/site-packages (0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install googledrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path_dir = str(Path.home()) + \"/data/2020/\"  # ustawmy sciezke na HOME/data/2020\n",
    "archive_dir = path_dir + \"survey.zip\"        # plik zapiszemy pod nazwa survey.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/data/2020/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/data/2020/survey.zip'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sciagniecie pliku we wskazane miejsce\n",
    "gdd.download_file_from_google_drive(file_id='1dfGerWeWkcyQ9GX9x20rdSGj7WtEpzBB',\n",
    "                                    dest_path=archive_dir,\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 1 </span>\n",
    "Zapoznaj sie z plikami tekstowymi (survey_results_public.csv oraz survey_results_schema.csv). Podejrzyj ich zawartość, sprawdź ich wielkość (liczba linii oraz rozmiar). Wgraj plik do swojego kubełka `gs://ds-$SEMESTER-$USER_ID-notebook-data` na GCS do katalogu `survey/2020/`. Jesli nie masz kubełka stwórz go.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USER_ID=7076\n",
      "env: SEMESTER=2024l\n"
     ]
    }
   ],
   "source": [
    "%env USER_ID=7076\n",
    "%env SEMESTER=2024l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///root/data/2020/survey_results_public.csv [Content-Type=text/csv]...\n",
      "- [1 files][ 90.2 MiB/ 90.2 MiB]                                                \n",
      "Operation completed over 1 objects/90.2 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp /root/data/2020/survey_results_public.csv gs://ds-$SEMESTER-$USER_ID-notebook-data/survey/2020/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Podłączenie do sesji Spark na GKE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WAZNE\n",
    "jesli w poprzednim notatniku masz aktywną sesję Spark zakończ ją (w poprzednim notatniku) poleceniem spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/22 15:16:52 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/12/22 15:16:52 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/12/22 15:16:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/12/22 15:16:53 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#spark.stop()\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config(\"spark.executor.instances\", \"1\")\\\n",
    ".config('spark.driver.memory','1g')\\\n",
    ".config('spark.executor.memory', '1g') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dostęp do danych na GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ścieżka dostępu do pliku na GCS\n",
    "# TODO Please update user id\n",
    "user_id = 7076\n",
    "semester = '2024l'\n",
    "gs_path = f'gs://ds-{semester}-{user_id}-notebook-data/survey/2020/survey_results_public.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://ds-2024l-7076-notebook-data/survey/2020/survey_results_public.csv'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "Platforma Apache Spark posiada komponent Spark SQL, który pozwala traktować dane jak tabele w bazie danych. Można zakładać swoje schematy baz danych oraz korzystać z języka SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_name = \"survey_2020\"                               # nazwa tabeli ktora bedziemy chcieli stworzyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "24/12/22 15:19:36 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`default`.`survey_2020` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/12/22 15:19:36 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f'DROP TABLE IF EXISTS {table_name}')       # usun te tabele jesli istniala wczesniej \n",
    "\n",
    "# stworz tabele korzystajac z danych we wskazanej lokalizacji\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
    "          USING csv \\\n",
    "          OPTIONS (HEADER true, INFERSCHEMA true) \\\n",
    "          LOCATION \"{gs_path}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weryfikacja danych \n",
    "Sprawdzmy zaczytane dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|          Respondent|      int|   NULL|\n",
      "|          MainBranch|   string|   NULL|\n",
      "|            Hobbyist|   string|   NULL|\n",
      "|                 Age|   string|   NULL|\n",
      "|          Age1stCode|   string|   NULL|\n",
      "|            CompFreq|   string|   NULL|\n",
      "|           CompTotal|   string|   NULL|\n",
      "|       ConvertedComp|   string|   NULL|\n",
      "|             Country|   string|   NULL|\n",
      "|        CurrencyDesc|   string|   NULL|\n",
      "|      CurrencySymbol|   string|   NULL|\n",
      "|DatabaseDesireNex...|   string|   NULL|\n",
      "|  DatabaseWorkedWith|   string|   NULL|\n",
      "|             DevType|   string|   NULL|\n",
      "|             EdLevel|   string|   NULL|\n",
      "|          Employment|   string|   NULL|\n",
      "|           Ethnicity|   string|   NULL|\n",
      "|              Gender|   string|   NULL|\n",
      "|          JobFactors|   string|   NULL|\n",
      "|              JobSat|   string|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"describe {table_name}\").show() # nie wszystkie dane ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------+-------+\n",
      "|col_name                    |data_type|comment|\n",
      "+----------------------------+---------+-------+\n",
      "|Respondent                  |int      |NULL   |\n",
      "|MainBranch                  |string   |NULL   |\n",
      "|Hobbyist                    |string   |NULL   |\n",
      "|Age                         |string   |NULL   |\n",
      "|Age1stCode                  |string   |NULL   |\n",
      "|CompFreq                    |string   |NULL   |\n",
      "|CompTotal                   |string   |NULL   |\n",
      "|ConvertedComp               |string   |NULL   |\n",
      "|Country                     |string   |NULL   |\n",
      "|CurrencyDesc                |string   |NULL   |\n",
      "|CurrencySymbol              |string   |NULL   |\n",
      "|DatabaseDesireNextYear      |string   |NULL   |\n",
      "|DatabaseWorkedWith          |string   |NULL   |\n",
      "|DevType                     |string   |NULL   |\n",
      "|EdLevel                     |string   |NULL   |\n",
      "|Employment                  |string   |NULL   |\n",
      "|Ethnicity                   |string   |NULL   |\n",
      "|Gender                      |string   |NULL   |\n",
      "|JobFactors                  |string   |NULL   |\n",
      "|JobSat                      |string   |NULL   |\n",
      "|JobSeek                     |string   |NULL   |\n",
      "|LanguageDesireNextYear      |string   |NULL   |\n",
      "|LanguageWorkedWith          |string   |NULL   |\n",
      "|MiscTechDesireNextYear      |string   |NULL   |\n",
      "|MiscTechWorkedWith          |string   |NULL   |\n",
      "|NEWCollabToolsDesireNextYear|string   |NULL   |\n",
      "|NEWCollabToolsWorkedWith    |string   |NULL   |\n",
      "|NEWDevOps                   |string   |NULL   |\n",
      "|NEWDevOpsImpt               |string   |NULL   |\n",
      "|NEWEdImpt                   |string   |NULL   |\n",
      "|NEWJobHunt                  |string   |NULL   |\n",
      "|NEWJobHuntResearch          |string   |NULL   |\n",
      "|NEWLearn                    |string   |NULL   |\n",
      "|NEWOffTopic                 |string   |NULL   |\n",
      "|NEWOnboardGood              |string   |NULL   |\n",
      "|NEWOtherComms               |string   |NULL   |\n",
      "|NEWOvertime                 |string   |NULL   |\n",
      "|NEWPurchaseResearch         |string   |NULL   |\n",
      "|NEWPurpleLink               |string   |NULL   |\n",
      "|NEWSOSites                  |string   |NULL   |\n",
      "|NEWStuck                    |string   |NULL   |\n",
      "|OpSys                       |string   |NULL   |\n",
      "|OrgSize                     |string   |NULL   |\n",
      "|PlatformDesireNextYear      |string   |NULL   |\n",
      "|PlatformWorkedWith          |string   |NULL   |\n",
      "|PurchaseWhat                |string   |NULL   |\n",
      "|Sexuality                   |string   |NULL   |\n",
      "|SOAccount                   |string   |NULL   |\n",
      "|SOComm                      |string   |NULL   |\n",
      "|SOPartFreq                  |string   |NULL   |\n",
      "|SOVisitFreq                 |string   |NULL   |\n",
      "|SurveyEase                  |string   |NULL   |\n",
      "|SurveyLength                |string   |NULL   |\n",
      "|Trans                       |string   |NULL   |\n",
      "|UndergradMajor              |string   |NULL   |\n",
      "|WebframeDesireNextYear      |string   |NULL   |\n",
      "|WebframeWorkedWith          |string   |NULL   |\n",
      "|WelcomeChange               |string   |NULL   |\n",
      "|WorkWeekHrs                 |string   |NULL   |\n",
      "|YearsCode                   |string   |NULL   |\n",
      "|YearsCodePro                |string   |NULL   |\n",
      "+----------------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"describe {table_name}\").show(100, truncate=False) # niepoprawne typy danych... \"NA\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|Age|\n",
      "+---+\n",
      "| NA|\n",
      "| 99|\n",
      "| 98|\n",
      "| 97|\n",
      "| 96|\n",
      "| 95|\n",
      "| 94|\n",
      "| 89|\n",
      "| 88|\n",
      "| 86|\n",
      "| 85|\n",
      "| 84|\n",
      "| 83|\n",
      "| 81|\n",
      "| 80|\n",
      "| 79|\n",
      "| 78|\n",
      "| 77|\n",
      "| 76|\n",
      "| 75|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT DISTINCT Age FROM {table_name} ORDER BY Age DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsługa wartosci 'NA' - ponowne stworzenie tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/22 15:23:10 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`default`.`survey_2020` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "\n",
    "# wykorzystujemy dodatkową opcję: NULLVALUE \"NA\"\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
    "          USING csv \\\n",
    "          OPTIONS (HEADER true, INFERSCHEMA true, NULLVALUE \"NA\") \\\n",
    "          LOCATION \"{gs_path}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|          Respondent|      int|   NULL|\n",
      "|          MainBranch|   string|   NULL|\n",
      "|            Hobbyist|   string|   NULL|\n",
      "|                 Age|   double|   NULL|\n",
      "|          Age1stCode|   string|   NULL|\n",
      "|            CompFreq|   string|   NULL|\n",
      "|           CompTotal|   double|   NULL|\n",
      "|       ConvertedComp|   double|   NULL|\n",
      "|             Country|   string|   NULL|\n",
      "|        CurrencyDesc|   string|   NULL|\n",
      "|      CurrencySymbol|   string|   NULL|\n",
      "|DatabaseDesireNex...|   string|   NULL|\n",
      "|  DatabaseWorkedWith|   string|   NULL|\n",
      "|             DevType|   string|   NULL|\n",
      "|             EdLevel|   string|   NULL|\n",
      "|          Employment|   string|   NULL|\n",
      "|           Ethnicity|   string|   NULL|\n",
      "|              Gender|   string|   NULL|\n",
      "|          JobFactors|   string|   NULL|\n",
      "|              JobSat|   string|   NULL|\n",
      "|             JobSeek|   string|   NULL|\n",
      "|LanguageDesireNex...|   string|   NULL|\n",
      "|  LanguageWorkedWith|   string|   NULL|\n",
      "|MiscTechDesireNex...|   string|   NULL|\n",
      "|  MiscTechWorkedWith|   string|   NULL|\n",
      "|NEWCollabToolsDes...|   string|   NULL|\n",
      "|NEWCollabToolsWor...|   string|   NULL|\n",
      "|           NEWDevOps|   string|   NULL|\n",
      "|       NEWDevOpsImpt|   string|   NULL|\n",
      "|           NEWEdImpt|   string|   NULL|\n",
      "|          NEWJobHunt|   string|   NULL|\n",
      "|  NEWJobHuntResearch|   string|   NULL|\n",
      "|            NEWLearn|   string|   NULL|\n",
      "|         NEWOffTopic|   string|   NULL|\n",
      "|      NEWOnboardGood|   string|   NULL|\n",
      "|       NEWOtherComms|   string|   NULL|\n",
      "|         NEWOvertime|   string|   NULL|\n",
      "| NEWPurchaseResearch|   string|   NULL|\n",
      "|       NEWPurpleLink|   string|   NULL|\n",
      "|          NEWSOSites|   string|   NULL|\n",
      "|            NEWStuck|   string|   NULL|\n",
      "|               OpSys|   string|   NULL|\n",
      "|             OrgSize|   string|   NULL|\n",
      "|PlatformDesireNex...|   string|   NULL|\n",
      "|  PlatformWorkedWith|   string|   NULL|\n",
      "|        PurchaseWhat|   string|   NULL|\n",
      "|           Sexuality|   string|   NULL|\n",
      "|           SOAccount|   string|   NULL|\n",
      "|              SOComm|   string|   NULL|\n",
      "|          SOPartFreq|   string|   NULL|\n",
      "|         SOVisitFreq|   string|   NULL|\n",
      "|          SurveyEase|   string|   NULL|\n",
      "|        SurveyLength|   string|   NULL|\n",
      "|               Trans|   string|   NULL|\n",
      "|      UndergradMajor|   string|   NULL|\n",
      "|WebframeDesireNex...|   string|   NULL|\n",
      "|  WebframeWorkedWith|   string|   NULL|\n",
      "|       WelcomeChange|   string|   NULL|\n",
      "|         WorkWeekHrs|   double|   NULL|\n",
      "|           YearsCode|   string|   NULL|\n",
      "|        YearsCodePro|   string|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE {table_name}\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  Age|\n",
      "+-----+\n",
      "|279.0|\n",
      "| 99.0|\n",
      "| 98.0|\n",
      "| 97.0|\n",
      "| 96.0|\n",
      "| 95.0|\n",
      "| 94.0|\n",
      "| 89.0|\n",
      "| 88.0|\n",
      "| 86.0|\n",
      "| 85.0|\n",
      "| 84.0|\n",
      "| 83.0|\n",
      "| 81.0|\n",
      "| 80.0|\n",
      "| 79.0|\n",
      "| 78.0|\n",
      "| 77.0|\n",
      "| 76.0|\n",
      "| 75.0|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT DISTINCT  Age FROM {table_name} ORDER BY Age DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   64461|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# sprawdzenie liczności tabeli\n",
    "spark.sql(f\"select count(*) from {table_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[count(1)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=211]\n",
      "      +- HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "         +- FileScan csv spark_catalog.default.survey_2020[] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[gs://ds-2024l-7076-notebook-data/survey/2020/survey_results_public.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select count(*) from {table_name}\").explain()  # tak jak na poprzednich zajeciach mozemy wygenerowac plany wykonania polecenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podgląd danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from {table_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biblioteka Pandas\n",
    "\n",
    "https://pandas.pydata.org/\n",
    "\n",
    "Moduł Pandas jest biblioteką Pythonową do manipulacji danymi. W szczegolnosci w pandas mozemy stworzyc ramki danych i wykonywac na niej analize, agregacje oraz wizualizacje danych. \n",
    "Przy nieduzych zbiorach danych i prostych operacjach to doskonała biblioteka. Jednak kiedy zbior danych sie rozrasta lub kiedy wymagane sa zlozone transformacje to operacje moga byc wolne.\n",
    "\n",
    "Operacje na rozproszonych danych sa szybsze. Ale tu takze napotykamy ograniczenia np trudność w wizualizacji danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from {table_name} limit 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ważne** \n",
    "\n",
    "Metoda toPandas() na ramce pyspark, konwertuje ramkę pyspark do ramki pandas. Wykonuje akcje pobrania (collect) wszystkich danych z executorów (z JVM) i transfer do  programu sterujacego (driver) i konwersje do typu Pythonowego w notatniku. Ze względu na ograniczenia pamięciowe w programie sterującym należy to wykonywać na podzbiorach danych.\n",
    "\n",
    "**DataFrame.collect() collects the distributed data to the driver side as the local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side.**\n",
    "\n",
    "**Note that DataFrame.toPandas() results in the collection of all records in the DataFrame to the driver program and should be done on a small subset of the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df = spark.sql(f\"select * from {table_name} LIMIT 10\")\n",
    "local_df = spark.sql(f\"select * from {table_name} LIMIT 10\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dist_df)  # dataframe Sparkowy (\"przepis na dane, rozproszony, leniwy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(local_df)  # dataframe Pandasowy (lokalny, sciągnięty do pamięci operacyjnej)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)    # pokazuj wszystkie kolumny\n",
    "# pd.reset_option(“display.max_columns”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 2 </span>\n",
    "Napisz w Spark SQL zapytanie które zwróci średnią liczbę godzin przepracowywanych przez z respondentów pogrupowanych ze względu na kraj. Następnie przekształć wynik do ramki pandasowej i ją wyświetl.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wizualizacje\n",
    "\n",
    "Do wizualizacji będziemy się posługiwać modułami matplotlib (https://matplotlib.org/) i seaborn (https://seaborn.pydata.org/). Do bardzo rozbudowane moduły, zachęcamy do eksploracji oficjalnych dokumentacji. Na zajęciach zrealizujemy następujące wykresy:\n",
    "* histogramy\n",
    "* liniowe \n",
    "* wiolinowe\n",
    "* kołowe \n",
    "\n",
    "Moduły wizualizacyjne wymagają danych na lokalnej maszynie. Mogą być to natywne typy danych Pythonowe (słowniki, listy) ale także np ramki danych pandasowe. ~~Nie działa wizualizacja na ramkach danych Sparkowych.~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj histogram wieku respondentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie danych\n",
    "# przycinamy dane tylko do zakresu ktory jest potrzebny do realizacji polecenia\n",
    "ages = spark.sql(f\"SELECT CAST (Age AS INT) \\\n",
    "                    FROM {table_name} \\\n",
    "                    WHERE age IS NOT NULL \\\n",
    "                    AND age BETWEEN 10 AND 80\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages.hist(\"Age\", bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(ages, bins=10, rug=True, kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaki jest udział programistów hobbistów? Narysuj wykres kołowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będzie nas interesowała ta proporcja ze względu na płeć."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (filtrowanie, grupowanie i zliczenie) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "hobby_all = spark.sql(f\"SELECT Hobbyist, COUNT(*) AS cnt FROM {table_name} WHERE Hobbyist IS NOT NULL GROUP BY Hobbyist\").toPandas()\n",
    "hobby_men = spark.sql(f\"SELECT Hobbyist, COUNT(*) AS cnt FROM {table_name} WHERE Hobbyist IS NOT NULL AND Gender='Man' GROUP BY Hobbyist\").toPandas()\n",
    "hobby_women = spark.sql(f\"SELECT Hobbyist, COUNT(*) AS cnt FROM {table_name} WHERE Hobbyist IS NOT NULL AND Gender='Woman' GROUP BY Hobbyist\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobby_all.plot.pie(y='cnt', labels=hobby_all['Hobbyist'], title=\"All\", autopct='%.0f')\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobby_men.plot.pie(y='cnt', labels=hobby_men['Hobbyist'], title=\"Men\", autopct='%.0f')\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobby_women.plot.pie(y='cnt', labels=hobby_women['Hobbyist'], title=\"Women\", autopct='%.0f')\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "hobby_all.plot.pie(y='cnt', labels=hobby_all['Hobbyist'], title=\"All\", ax=axes[0], autopct='%.0f')\n",
    "hobby_men.plot.pie(y='cnt', labels=hobby_men['Hobbyist'], title=\"Men\", ax=axes[1], autopct='%.0f')\n",
    "hobby_women.plot.pie(y='cnt', labels=hobby_men['Hobbyist'], title=\"Women\", ax=axes[2], autopct='%.0f')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykres liniowy. Zależność między wiekiem a liczbą przepracowanych godzin\n",
    "Interesują nas dla developerzy profesjonaliści (nie hobbiści) w przedziale wiekowym 18-65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (filtrowanie, grupowanie, wyliczenie sredniej oraz sortowanie) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "age_work = spark.sql(f\"SELECT age, CAST (avg(WorkWeekHrs) AS INT) AS avg FROM {table_name} \\\n",
    "            WHERE WorkWeekHrs IS NOT NULL AND age BETWEEN 18 AND 65 AND hobbyist = 'No' \\\n",
    "            GROUP BY age \\\n",
    "            ORDER BY age ASC\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_work.plot(x='age', y='avg', kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"age\", y=\"avg\", kind=\"line\", data=age_work);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykres słupkowy. Pokaż liczbę respondentów na kraj\n",
    "\n",
    "Interesuje nas tylko 10 krajów o najwyższej liczbie respondentow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (grupowanie, zliczenie, sortowanie oraz przyciecie do 10 wyników) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "max_countries = spark.sql(f\"SELECT country, COUNT(*) AS cnt \\\n",
    "                FROM {table_name} \\\n",
    "                GROUP BY country \\\n",
    "                ORDER BY cnt DESC \\\n",
    "                LIMIT 10 \").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_countries.plot.bar(y='cnt', x='country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"country\", y=\"cnt\", kind=\"bar\",\\\n",
    "            data=max_countries).set_xticklabels(rotation=65)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykres słupkowy. Średnie zarobki w  krajach w ktorych jest powyzej 1000 respondentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (filtrowanie, grupowanie, wyliczenie sredniej, filtrowanie po liczności i grup oraz sortowanie) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "country_salary = spark.sql(f\"SELECT country, \\\n",
    "    CAST (avg(ConvertedComp) AS INT) as avg \\\n",
    "    FROM {table_name} \\\n",
    "    WHERE country IS NOT NULL \\\n",
    "    GROUP BY country \\\n",
    "    HAVING COUNT(*) > 1000 \\\n",
    "    ORDER BY avg DESC \").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_salary.plot.barh((\"country\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot. Pokaz rozklad pensji w krajach gdzie jest powyzej 1000 respondentów\n",
    "Tutaj będziemy musieli skorzystać z podzapytania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie danych na rozproszonych danych (spark sql). Mamy tu do czynienia z podzapytaniem\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "country_comp = spark.sql(f\"SELECT country, CAST(ConvertedComp AS INT) \\\n",
    "                FROM {table_name} \\\n",
    "                WHERE country IN (SELECT country FROM {table_name} GROUP BY country HAVING COUNT(*) > 1000) \\\n",
    "                AND ConvertedComp IS NOT NULL AND ConvertedComp > 0 \\\n",
    "                ORDER BY ConvertedComp desc\").toPandas()\n",
    "country_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_comp.boxplot(column=\"ConvertedComp\", by=\"country\", \\\n",
    "                     showfliers=False, rot=60, meanline=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"country\", y=\"ConvertedComp\", kind=\"box\", \\\n",
    "            showfliers=False, data=country_comp, palette=\"Blues\")\\\n",
    "    .set_xticklabels(rotation=65)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 3 </span>\n",
    "Narysuj rozklad pensji w zaleznosci od plci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres popularnosci jezykow programowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select LanguageWorkedWith from {table_name} where LanguageWorkedWith IS NOT NULL\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Języki programowania są zapisane w pojedynczej komórce. Będzie trzeba je rozdzielić i zliczyć. Tak przygotowane dane dopiero posłużą nam do narysowania wykresu. Wykorzystamy funkcję `posexplode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = spark.sql(f\"select LanguageWorkedWith from {table_name} where LanguageWorkedWith IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "langs.select(\n",
    "        posexplode(split(\"LanguageWorkedWith\", \";\")).alias(\"pos\", \"language\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs.select(\n",
    "        posexplode(split(\"LanguageWorkedWith\", \";\")).alias(\"pos\", \"language\")).groupBy(\"language\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_pd = langs.select(\n",
    "        posexplode(split(\"LanguageWorkedWith\", \";\")).alias(\"pos\", \"language\")).groupBy(\"language\").count().orderBy(\"count\").toPandas()\n",
    "langs_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8, 9))\n",
    "plt.barh(width=langs_pd[\"count\"], y=langs_pd[\"language\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres popularnosci jezykow wsrod Data Scientists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdefiniujmy sobie funkcję, która przekształca nam języki do wymaganej przez nas postaci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def prepare_lang(df, colName='LanguageWorkedWith'):\n",
    "    summary = df.select(posexplode(split(colName, \";\")).alias(\"pos\", \"language\")).groupBy(\"language\").count().orderBy(\"count\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_ds = spark.sql(f\"SELECT LanguageWorkedWith \\\n",
    "                FROM {table_name} \\\n",
    "                WHERE DevType LIKE '%Data scientist%'\")\n",
    "\n",
    "sum_lang = prepare_lang(langs_ds).toPandas()\n",
    "\n",
    "figure(figsize=(8, 9))\n",
    "plt.barh(width=sum_lang[\"count\"], y=sum_lang[\"language\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres którego chcą wykorzystywać w przyszłości Data Scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_desired = spark.sql(f\"select LanguageDesireNextYear \\\n",
    "                from {table_name} \\\n",
    "                where DevType like '%Data scientist%'\")\n",
    "\n",
    "sum_lang = prepare_lang(lang_desired, 'LanguageDesireNextYear').toPandas()\n",
    "\n",
    "figure(num=None, figsize=(8, 9), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.barh(width=sum_lang[\"count\"], y=sum_lang[\"language\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres prezentujący liczbę godzin na pracy w zależności od wykształcenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select distinct EdLevel from {table_name}\").show(truncate=False) # jakie są wartości wykształcenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ed_pandas = spark.sql(f\"SELECT EdLevel, WorkWeekHrs FROM {table_name} \\\n",
    "            WHERE WorkWeekHrs BETWEEN 10 AND 80 \\\n",
    "            AND (EdLevel LIKE '%Bachelor%' OR EdLevel LIKE '%Master%' OR EdLevel LIKE '%Other doctoral%')\").toPandas()\n",
    "\n",
    "ed_pandas['EdLevel'] = ed_pandas['EdLevel'].replace('Bachelor’s degree (B.A., B.S., B.Eng., etc.)','Bachelor')\n",
    "ed_pandas['EdLevel'] = ed_pandas['EdLevel'].replace('Master’s degree (M.A., M.S., M.Eng., MBA, etc.)','Master')\n",
    "ed_pandas['EdLevel'] = ed_pandas['EdLevel'].replace('Other doctoral degree (Ph.D., Ed.D., etc.)','Doctor')\n",
    "\n",
    "ed_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"EdLevel\", y=\"WorkWeekHrs\", data=ed_pandas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres wiolinowy pokazujacy rozkład dochodów w zależności od wykształcenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_pay = spark.sql(f\"SELECT EdLevel, CAST (CompTotal AS INT) AS CompTotal FROM {table_name} \\\n",
    "            WHERE CompTotal BETWEEN 0 AND 1000000  \\\n",
    "            AND (EdLevel LIKE '%Bachelor%' OR EdLevel LIKE '%Master%' OR EdLevel LIKE '%Other doctoral%')\").toPandas()\n",
    "\n",
    "ed_pay['EdLevel'] = ed_pay['EdLevel'].replace('Bachelor’s degree (B.A., B.S., B.Eng., etc.)','Bachelor')\n",
    "ed_pay['EdLevel'] = ed_pay['EdLevel'].replace('Master’s degree (M.A., M.S., M.Eng., MBA, etc.)','Master')\n",
    "ed_pay['EdLevel'] = ed_pay['EdLevel'].replace('Other doctoral degree (Ph.D., Ed.D., etc.)','Doctor')\n",
    "\n",
    "ed_pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"EdLevel\", y=\"CompTotal\", kind=\"boxen\",\n",
    "            data=ed_pay);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ Narysuj heatmape odwiedzin na StackOverflow dla wybranych krajów ⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT DISTINCT SOVisitFreq FROM {table_name}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_v = spark.sql(f\"SELECT SOVisitFreq, t1.country, COUNT(*)/first(t2.t) AS cnt from {table_name} t1 \\\n",
    "            JOIN (SELECT country, COUNT(*) as t FROM {table_name} GROUP BY country) t2 \\\n",
    "            ON t1.country = t2.country \\\n",
    "            WHERE t1.country IS NOT NULL AND SOVisitFreq IS NOT NULL \\\n",
    "            AND t1.country IN ('Poland', 'United States', 'Russian Federation', 'China', 'India', 'Germany', 'Japan') \\\n",
    "            GROUP BY t1.country, SOVisitFreq\").toPandas()\n",
    "\n",
    "so_v['SOVisitFreq'] = pd.Categorical(so_v['SOVisitFreq'], [\"I have never visited Stack Overflow (before today)\", \"Less than once per month or monthly\", \"A few times per month or weekly\", \"A few times per week\", \"Daily or almost daily\", \"Multiple times per day\"])\n",
    "# so_v.sort_values['SOVisitFreq']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap2_data = pd.pivot_table(so_v, values='cnt', index=['country'], columns='SOVisitFreq')\n",
    "sns.heatmap(heatmap2_data, cmap=\"BuGn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 4 </span>\n",
    "* Narysuj wykres słupkowy popularności wykorzystywanych baz danych przez profesjonalnych programistów.\n",
    "* Narysuj wykres kołowy przedstawiający procentowy udział poziomu wykształcenia inz, mgr i dr w grupie respondentów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "notebook_test": {
   "keytab_path": "/data/work/home/ds-lab-testuser1/ds-lab-testuser1.keytab",
   "user": "ds-lab-testuser1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
